{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Linear System Solvers\n",
    "\n",
    "We cover some topics from Ch. 3 and 4 of Quatteroni (2000) about\n",
    "- Direct solvers: Gauss elimination method\n",
    "- Iterative solvers: Jacobi, Gauss-Seidel, Conjugate-Gradient, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stability Analysis\n",
    "\n",
    "We open this unit with some useful facts about the stability of a linear system. \n",
    "\n",
    "- Usually a subject in applied mathematics \n",
    "- Useful concepts are needed for and derived from the analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The condition number of a matrix\n",
    "\n",
    "For $A \\in \\mathbb{C}^{n\\times n}$, the condition nmumber of $A$ is defined as\n",
    "\\begin{equation}\n",
    "K(\\mathbf{A}) = \\Vert A \\Vert \\Vert A^{-1} \\Vert,\n",
    "\\end{equation}\n",
    "where $\\Vert \\cdot \\Vert $ is an induced matrix norm.\n",
    "\n",
    "A norm of a general $m\\times n$ matrix is, in general, defined as\n",
    "\\begin{equation}\n",
    "\\sup_{\\mathbf{v}\\in \\mathbb{C}^{n}\\\\\\mathbf{v} \\neq 0}\\frac{\\Vert \\mathbf{Av}\\Vert}{\\Vert\\mathbf{v}\\Vert}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Some frequently used norms\n",
    "\n",
    "- $\\Vert \\cdot \\Vert_{1}$: Column-sum norm\n",
    "\\begin{equation}\n",
    "\\Vert \\mathbf{A} \\Vert_{1} = \\max_{j=1,\\ldots,n}\\sum_{i=1}^{m}\\vert a_{ij} \\vert,\n",
    "\\end{equation}\n",
    "\n",
    "- $\\Vert \\cdot \\Vert_{\\infty}$: Row-sum norm\n",
    "\\begin{equation}\n",
    "\\Vert \\mathbf{A} \\Vert_{\\infty} = \\max_{i=1,\\ldots,n}\\sum_{j=1}^{m}\\vert a_{ij} \\vert,\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- $\\Vert \\cdot \\Vert_{2}$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\Vert \\mathbf{A} \\Vert_{2} &= \\sqrt{\\rho(\\mathbf{A}^{H}\\mathbf{A})} = \\sqrt{\\rho(\\mathbf{A}\\mathbf{A}^{H})} \\\\\n",
    "&= \\sigma_{1}(\\mathbf{A}),\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "where $\\sigma_{1}(\\mathbf{A})$ is the maximum singular value of $\\mathbf{A}$\n",
    "- When $\\mathbf{A}$ is real and symmetric (i.e., $\\mathbf{A}^{H}=\\mathbf{A}$),\n",
    "    $\\Vert \\mathbf{A} \\Vert_{2} = \\rho(\\mathbf{A})$.\n",
    "    - $\\rho(\\mathbf{A})$ is called **spectral radius of $\\mathbf{A}$** and defined as\n",
    "    \\begin{equation}\n",
    "    \\rho(\\mathbf{A}) = \\max_{\\lambda \\in \\sigma(\\mathbf{A})} |\\lambda|\n",
    "    \\end{equation}\n",
    "    - $\\sigma(\\mathbf{A})$ above is the set of eigenvalues of $\\mathbf{A}$ and called the **spectrum of $\\mathbf{A}$**.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Examples of matrix condition numbers\n",
    "- $K_{\\infty}(\\mathbf{A})=\\Vert \\mathbf{A} \\Vert_{\\infty}\\Vert \\mathbf{A}^{-1} \\Vert_{\\infty}$\n",
    "- $K_{2}(\\mathbf{A})=\\Vert \\mathbf{A} \\Vert_{2}\\Vert \\mathbf{A}^{-1} \\Vert_{2}$\n",
    "\n",
    "Some other facts about include\n",
    "- $1=\\Vert \\mathbf{A}\\mathbf{A}^{-1} \\Vert \\le \\Vert \\mathbf{A} \\Vert \\Vert \\mathbf{A}^{-1} \\Vert = K(\\mathbf{A}) \\quad \\therefore K(\\mathbf{A}) \\ge 1$.\n",
    "- $K(\\mathbf{A}^{-1}) = K(\\mathbf{A})$.\n",
    "- $^{\\forall}\\alpha \\in \\mathbb{C} \\text{ with } \\alpha \\neq 0,\\ K(\\alpha\\mathbf{A})=K(\\mathbf{A})$.\n",
    "- \n",
    "\\begin{equation}\n",
    "K_{2}(\\mathbf{A}) = \\Vert \\mathbf{A} \\Vert_{2} \\Vert \\mathbf{A}^{-1} \\Vert_{2} = \\frac{\\sigma_{1}(\\mathbf{A})}{\\sigma_{n}(\\mathbf{A})}\n",
    "\\end{equation}\n",
    "- If $\\mathbf{A}$ is symmetric, positive definite (i.e., $\\lambda_{i} > 0$),\n",
    "\\begin{equation}\n",
    "K_{2}(\\mathbf{A}) = \\frac{\\lambda_{max}}{\\lambda_{min}}=\\rho(\\mathbf{A})\\rho(\\mathbf{A}^{-1}).\n",
    "\\end{equation}\n",
    "    - $K_{2}$ is called **spectral condition number**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### How far a matrix is from singularity\n",
    "\n",
    "- Why do we care? \n",
    "    - Because $\\mathbf{Ax}=\\mathbf{0}$ does not necessarily mean $\\mathbf{x}=\\mathbf{0}$ if $\\mathbf{A}$ is singular.\n",
    "    \n",
    "The relative distance of $\\mathbf{A} \\in \\mathbb{C}^{n\\times n}$ from the set of singular matrix with respect to the $p$-norm is defined as\n",
    "\\begin{equation}\n",
    "\\text{dist}_{p}(\\mathbf{A}) = \\min\\left\\{ \\frac{\\Vert \\delta \\mathbf{A} \\Vert_{p}}{\\Vert \\mathbf{A} \\Vert_{p}}: \\mathbf{A} + \\delta\\mathbf{A}\\text{ is singular}. \\right\\}\n",
    "\\end{equation}\n",
    "\n",
    "It can be shown that $\\text{dist}_{p}(\\mathbf{A}) = 1/K_{p}(\\mathbf{A})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Numpy Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  0 -1]\n",
      " [ 0  1  0]\n",
      " [ 1  0  1]]\n",
      "2.0\n",
      "1.4142135623730951\n",
      "[1.41421356 1.41421356 1.        ]\n",
      "2.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([[1, 0, -1], [0, 1, 0], [1, 0, 1]])\n",
    "b = np.array([[5.0,0.0,0.0],[2.0,6.0,0.0],[2.0,1.0,8.0]]) # 3x3 matrix\n",
    "print(a)\n",
    "print(np.linalg.cond(a,1)) # max column sum\n",
    "print(np.linalg.cond(a,2)) # max singular vaule\n",
    "u, s, vh = np.linalg.svd(a) # singular value decomposition of a\n",
    "print(s) # singular values of a\n",
    "print(np.linalg.cond(a,np.inf)) # max row sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Forward *a priori* Analysis\n",
    "\n",
    "- If a numerical method is used for solving $\\mathbf{Ax}=\\mathbf{b}$, the exact solution cannot be acquired due to rounding errors.\n",
    "- In other words, a numerical method yields an *exact* solution $\\mathbf{x}+\\delta\\mathbf{x}$ of the perturbed system\n",
    "\\begin{equation}\n",
    "(\\mathbf{A}+\\delta\\mathbf{A})(\\mathbf{x}+\\delta\\mathbf{x})=(\\mathbf{b}+\\delta\\mathbf{b})\n",
    "\\end{equation}\n",
    "- We would like to estimate $\\delta\\mathbf{x}$ in terms of $\\delta\\mathbf{A}$ and $\\delta\\mathbf{b}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- **Theorem**\n",
    "    - Let $\\mathbf{A} \\in \\mathbb{R}^{n\\times n}$ be a non-singular matrix and $\\delta \\mathbf{A} \\in \\mathbb{R}^{n\\times n}$ be such that $\\mathbf{A}+\\delta\\mathbf{A}$ is still non-singular.\n",
    "    - Then, if $\\mathbf{x} \\in \\mathbb{R}^{n}$ is the solution of $\\mathbf{Ax}=\\mathbf{b}$ with $\\mathbf{b} \\in \\mathbb{R}^{n}$ ($\\mathbf{b}\\neq \\mathbf{0}$) and $\\delta \\mathbf{x} \\in \\mathbb{R}^{n}$ satisfied the perturbed system for $\\delta \\mathbf{b} \\in \\mathbb{R}^{n}$,\n",
    "    \\begin{equation}\n",
    "      \\frac{\\Vert \\delta \\mathbf{x} \\Vert}{\\Vert \\mathbf{x} \\Vert} \\le \\frac{K(\\mathbf{A})}{1-K(\\mathbf{A})\\Vert \\delta \\mathbf{A}\\Vert / \\Vert \\mathbf{A} \\Vert} \\left( \\frac{\\Vert \\delta \\mathbf{b} \\Vert}{\\Vert \\mathbf{b} \\Vert} + \\frac{\\Vert \\delta \\mathbf{A} \\Vert}{\\Vert \\mathbf{A} \\Vert} \\right).\n",
    "    \\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- **Theorem**\n",
    "    - If $\\delta \\mathbf{A}=0$, \n",
    "    \\begin{equation}\n",
    "      \\frac{1}{K(\\mathbf{A})} \\frac{\\Vert \\delta \\mathbf{b} \\Vert}{\\Vert \\mathbf{b} \\Vert} \\le\n",
    "      \\frac{\\Vert \\delta \\mathbf{x} \\Vert}{\\Vert \\mathbf{x} \\Vert} \\le \n",
    "      K(\\mathbf{A}) \\frac{\\Vert \\delta \\mathbf{b} \\Vert}{\\Vert \\mathbf{b} \\Vert}\n",
    "    \\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### *a posteriori* Analysis\n",
    "- Analysis done **after** an approximation solution is acquired.\n",
    "- The aim is to relate unknown error $\\mathbf{e} = \\mathbf{y}-\\mathbf{x}$ to quantities that can be computed using $\\mathbf{y}$ and $\\mathbf{C}$ where $\\mathbf{C}$ is the approximate inverse of $\\mathbf{A}$ and $\\mathbf{y}$ is a known approximate solution from $\\mathbf{y}=\\mathbf{C}\\mathbf{b}$.\n",
    "    - cf. $\\mathbf{x}=\\mathbf{A}^{-1}\\mathbf{b}$.\n",
    "- Residual vector ($\\mathbf{r}$)\n",
    "\\begin{equation}\n",
    "\\mathbf{r} = \\mathbf{b} - \\mathbf{Ay},\n",
    "\\end{equation}\n",
    "    - In general, $\\mathbf{r}$ is non-zero.\n",
    "- Residual and error are related as follows:\n",
    "\\begin{equation}\n",
    "\\mathbf{e} = \\mathbf{y}-\\mathbf{x} = \\mathbf{A}^{-1}(\\mathbf{Ay}-\\mathbf{b}) = -\\mathbf{A}^{-1}\\mathbf{r}\n",
    "\\end{equation}\n",
    "- $\\mathbf{R}=\\mathbf{AC}-\\mathbf{I}$. If $\\Vert \\mathbf{R} \\Vert < 1$,\n",
    "\\begin{equation}\n",
    "\\Vert e \\Vert \\le \\frac{\\Vert \\mathbf{r} \\Vert\\,\\Vert \\mathbf{C} \\Vert}{1-\\Vert \\mathbf{R} \\Vert}.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Direct method\n",
    "\n",
    "- Solution is acquired in a **finite** number of steps. Usually for small problems.\n",
    "- Iterative methods: Solution acquired in **infinite** steps.\n",
    "    - Usually requires fewer steps than a direct method\n",
    "    - Often used for large problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Forward substitution\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{Lx} = \\mathbf{b}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{bmatrix}\n",
    "l_{11} & 0      & 0      & \\cdots \\\\\n",
    "l_{21} & l_{22} & 0      & \\cdots \\\\\n",
    "l_{31} & l_{32} & l_{33} & \\cdots \\\\\n",
    "       &   \\vdots &      &\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x_{1} \\\\\n",
    "x_{2} \\\\\n",
    "x_{3} \\\\\n",
    "\\vdots\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "b_{1} \\\\\n",
    "b_{2} \\\\\n",
    "b_{3} \\\\\n",
    "\\vdots\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{align}\n",
    "x_{1} &= b_{1}/l_{11} \\\\\n",
    "x_{2} &= (b_{2}-l_{21}x_{1})/l_{22} \\\\\n",
    "x_{3} &= (b_{3}-l_{31}x_{1}-l_{32}x_{2})/l_{33} \\\\\n",
    " &\\vdots \\\\\n",
    "x_{n} &= \\left( b_{n}-\\sum_{k=1}^{n-1}l_{nk}x_{k} \\right)/l_{nn} \\\\ \n",
    "\\end{align}\n",
    "\n",
    "- Numerical cost is $n^{2}$ flops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backward substitution\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{Ux} = \\mathbf{b}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{bmatrix}\n",
    "u_{11} & u_{12} & u_{13} & \\cdots & u_{1n} \\\\\n",
    "0      & u_{22} & u_{23} & \\cdots & u_{2n} \\\\\n",
    "0      & 0      & u_{33} & \\cdots & u_{3n} \\\\\n",
    "       &        & \\vdots &        &        \\\\\n",
    "0      & 0      & 0      & \\cdots & u_{nn}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x_{1} \\\\\n",
    "x_{2} \\\\\n",
    "x_{3} \\\\\n",
    "\\vdots \\\\\n",
    "x_{n}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "b_{1} \\\\\n",
    "b_{2} \\\\\n",
    "b_{3} \\\\\n",
    "\\vdots \\\\\n",
    "b_{n}\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{align}\n",
    "x_{n} &= b_{n}/u_{nn} \\\\\n",
    "x_{n-1} &= (b_{n-1}-u_{n-1\\,n}x_{n})/u_{n-1\\,n-1} \\\\\n",
    " &\\vdots \\\\\n",
    "x_{i} &= \\left( b_{i}-\\sum_{k=i+1}^{n}u_{ik}x_{k} \\right)/u_{ii}\n",
    "\\end{align}\n",
    "\n",
    "- Numerical cost is $n^{2}$ flops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\\begin{equation}\n",
    "x_{n} = \\left( b_{n}-\\sum_{k=1}^{n-1}l_{nk}x_{k} \\right)/l_{nn}\n",
    "\\end{equation}\n",
    "\n",
    "#### Program 1 - forward row : Forward substitution: row-oriented version\n",
    "```Matlab\n",
    "function [x]=forward_row(L,b)\n",
    "[n]=mat_square(L); x(1) = b(1)/L(1,1);\n",
    "for i = 2:n, x (i) = (b(i)-L(i,1:i-1)*(x(1:i-1))’)/L(i,i); end\n",
    "x=x’;\n",
    "```\n",
    "\n",
    "#### Program 2 - forward col : Forward substitution: column-oriented version\n",
    "```Matlab\n",
    "function [b]=forward_col(L,b)\n",
    "[n]=mat_square(L);\n",
    "for j=1:n-1,\n",
    "b(j)= b(j)/L(j,j); b(j+1:n)=b(j+1:n)-b(j)*L(j+1:n,j);\n",
    "end; b(n) = b(n)/L(n,n);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\\begin{equation}\n",
    "x_{i} = \\left( b_{i}-\\sum_{k=i+1}^{n}u_{ik}x_{k} \\right)/u_{ii}\n",
    "\\end{equation}\n",
    "\n",
    "#### Program 3 - backward col : Backward substitution: column-oriented version\n",
    "```Matlab\n",
    "function [b]=backward_col(U,b)\n",
    "[n]=mat_square(U);\n",
    "for j = n:-1:2,\n",
    "b(j)=b(j)/U(j,j); b(1:j-1)=b(1:j-1)-b(j)*U(1:j-1,j);\n",
    "end; b(1) = b(1)/U(1,1);\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L= [[5. 0. 0.]\n",
      " [2. 6. 0.]\n",
      " [2. 1. 8.]]\n",
      "x= [[0.1]\n",
      " [0.2]\n",
      " [0.3]]\n",
      "b= [[0.5]\n",
      " [1.4]\n",
      " [2.8]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "L = np.array([[5.0,0.0,0.0],[2.0,6.0,0.0],[2.0,1.0,8.0]]) # 3x3 matrix\n",
    "x = np.array([[0.1, 0.2, 0.3]]).T # Make x a column vector\n",
    "# Important!! \n",
    "# Matrix-vector multiplication as we know is \n",
    "#  np.dot(L,x) or L.dot(x)\n",
    "b = L.dot(x)  # L*x\n",
    "print('L=',L)\n",
    "print('x=',x)\n",
    "print('b=',b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.1]\n",
      " [0.2]\n",
      " [0.3]]\n"
     ]
    }
   ],
   "source": [
    "def forward_row(L,b):\n",
    "    # Check if L is a square matrix\n",
    "    n,m = L.shape\n",
    "    if n!=m:\n",
    "        print('L is not square')\n",
    "        return\n",
    "    x = np.zeros((n,1)) # column vector\n",
    "    x[0,0] = b[0,0]/L[0,0]\n",
    "    for i in range(1,n): # 1 to n-1\n",
    "        x[i,0] = (b[i,0]-L[i,0:i].dot(x[0:i,0]))/L[i,i]\n",
    "    return x\n",
    "print(forward_row(L,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gauss elimination\n",
    "\n",
    "- Reduces the system $\\mathbf{Ax}=\\mathbf{b}$ to an equivalent system of the form $\\mathbf{Ux}=\\hat{\\mathbf{b}}$.\n",
    "- Then, the reduced system can be solved by the backward substitution.\n",
    "- Complexity of GEM: $\\frac{2}{3}n^{3} + 2n^{2} \\sim \\frac{2}{3}n^{3}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- We review the algorithm using this example:\n",
    "\\begin{equation}\n",
    "\\mathbf{A}^{(1)} \\mathbf{x} = \\mathbf{b}^{(1)} \\quad\n",
    "\\begin{matrix}\n",
    "x_{1}  &+& \\frac{1}{2}x_{2} &+& \\frac{1}{3}x_{3} &=& \\frac{11}{6} \\\\\n",
    "\\frac{1}{2}x_{1} &+& \\frac{1}{3}x_{2} &+& \\frac{1}{4}x_{3} &=& \\frac{13}{12} \\\\\n",
    "\\frac{1}{3}x_{1} &+& \\frac{1}{4}x_{2} &+& \\frac{1}{5}x_{3} &=& \\frac{47}{60}\n",
    "\\end{matrix}\n",
    "\\end{equation}\n",
    "\n",
    "    - Go ahead and start solving this problem!\n",
    "    - Note that we want to make an upper triangular matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\\begin{equation}\n",
    "\\mathbf{A}^{(2)} \\mathbf{x} = \\mathbf{b}^{(2)} \\quad\n",
    "\\begin{matrix}\n",
    "x_{1}  &+& \\frac{1}{2}x_{2} &+& \\frac{1}{3}x_{3} &=& \\frac{11}{6} \\\\\n",
    "0  &+& \\frac{1}{12}x_{2} &+& \\frac{1}{12}x_{3} &=& \\frac{1}{6} \\\\\n",
    "0  &+& \\frac{1}{12}x_{2} &+& \\frac{4}{45}x_{3} &=& \\frac{31}{180}\n",
    "\\end{matrix}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{A}^{(3)} \\mathbf{x} = \\mathbf{b}^{(3)} \\quad\n",
    "\\begin{matrix}\n",
    "x_{1}  &+& \\frac{1}{2}x_{2} &+& \\frac{1}{3}x_{3} &=& \\frac{11}{6} \\\\\n",
    "0  &+& \\frac{1}{12}x_{2} &+& \\frac{1}{12}x_{3} &=& \\frac{1}{6} \\\\\n",
    "0  &+& 0 &+& \\frac{1}{180}x_{3} &=& \\frac{1}{180}\n",
    "\\end{matrix}\n",
    "\\end{equation}\n",
    "\n",
    "So, the solution is $x_{1}=x_{2}=x_{3}=1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Equivalence of GEM to LU factorization\n",
    "In the above example, note that $\\mathbf{A}^{(2)} = \\mathbf{M}_{1}\\mathbf{A}^{(1)}$, where\n",
    "\\begin{equation}\n",
    "\\mathbf{M}_{1} = \n",
    "\\begin{pmatrix}\n",
    "1 & 0 & 0 \\\\\n",
    "-\\frac{1}{2} & 1 & 0 \\\\\n",
    "-\\frac{1}{3} & 0 & 1 \\\\\n",
    "\\end{pmatrix}.\n",
    "\\end{equation}\n",
    "\n",
    "Also, $\\mathbf{A}^{(3)} = \\mathbf{M}_{2}\\mathbf{A}^{(2)} = \\mathbf{M}_{2}\\mathbf{M}_{1}\\mathbf{A}^{(1)} = \\mathbf{U}$, where\n",
    "\\begin{equation}\n",
    "\\mathbf{M}_{2} = \n",
    "\\begin{pmatrix}\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & -1 & 1 \\\\\n",
    "\\end{pmatrix}.\n",
    "\\end{equation}\n",
    "\n",
    "Therefore, we get this equivalence:\n",
    "\\begin{equation}\n",
    "\\mathbf{A} = \\left(\\mathbf{M}_{2}\\mathbf{M}_{1}\\right)^{-1}\\mathbf{U} = \\mathbf{LU}\n",
    "\\end{equation}\n",
    "because $\\mathbf{M}_{1}$ and $\\mathbf{M}_{2}$ are lower triangular matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- So, when $\\mathbf{Ax}=\\mathbf{b}$ is given, applying GEM is equivalent to perform LU factorization of $\\mathbf{A}$.\n",
    "\\begin{equation}\n",
    "\\mathbf{LUx}=\\mathbf{b}\n",
    "\\end{equation}\n",
    "    - Solve $\\mathbf{Ly}=\\mathbf{b}$ first adn then $\\mathbf{Ux}=\\mathbf{y}$ for $\\mathbf{x}$.\n",
    "- Special considerations are needed for **sparse** matrices\n",
    "    - $n\\times n$ sparse matrix has about $n$ non-zero elements: e.g., tridiagonal or other banded matrices.\n",
    "    - For instance, how to perform LU factorization in the *compact storage* form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Iterative Methods\n",
    "\n",
    "- The general idea is to construct a sequence of vectors, $\\mathbf{x}^{(k)}$ that has the property\n",
    "\\begin{equation}\n",
    "\\mathbf{x} = \\lim_{k\\rightarrow \\infty} \\mathbf{x}^{(k)},\n",
    "\\end{equation}\n",
    "where $\\mathbf{x}$ is the exact solution.\n",
    "- $\\mathbf{x}^{(0)}$ given, \n",
    "\\begin{equation}\n",
    "\\mathbf{x}^{(k+1)} = \\mathbf{B}\\mathbf{x}^{(k)} + \\mathbf{f}(\\mathbf{b}), \\quad k \\ge 0,\n",
    "\\end{equation}\n",
    "where $\\mathbf{B}$ is a $n\\times n$ square matrix called **iteration matrix**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- An iterative method of the above form is said to be **consistent** with $\\mathbf{Ax}=\\mathbf{b}$ if $\\mathbf{f}$ and $\\mathbf{B}$ are such that $\\mathbf{x} = \\mathbf{Bx} + \\mathbf{f}$.\n",
    "\n",
    "- The importance of iteration matrix is in that its **spectral radius** ($\\rho(\\mathbf{B})$) determines both **whether the method will converge** and **how fast it will converge** (i.e., convergence rate).\n",
    "\n",
    "#### Theorem 4.1 \n",
    "In the above iteration scheme, the sequence of vectors $\\left\\{ \\mathbf{x}^{(k)} \\right\\}$ converges to the solution of $\\mathbf{Ax}=\\mathbf{b}$ for any choice of $\\mathbf{x}^{(0)}$ iff $\\rho(\\mathbf{B}) < 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- **Asymptotic convergence rate and factor**\n",
    "\\begin{equation}\n",
    "R(\\mathbf{B}) = \\lim_{k\\rightarrow \\infty} R_{k}(\\mathbf{B}) = -\\log \\rho(\\mathbf{B}).\n",
    "\\end{equation}\n",
    "and $R(\\mathbf{B})$ is called *asymptotic convergence factor*.\n",
    "\n",
    "    - Note that the convergence rate is *positive* only when $\\rho(\\mathbf{B}) < 1$.\n",
    "    - In other words, if $\\rho(\\mathbf{B})\\ge 1$, the iteration method doesn't converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Linear iterative methods\n",
    "- *Linear* because these are based on **additive splitting** of $\\mathbf{A}$.\n",
    "- $\\mathbf{A} = \\mathbf{P} - \\mathbf{N}$, where $\\mathbf{P}$ is called **preconditioner**.\n",
    "- According to this decomposition, we get the following result:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{Ax}=\\mathbf{b} \\rightarrow (\\mathbf{P}-\\mathbf{N})x = \\mathbf{b} \\rightarrow \\mathbf{Px} = \\mathbf{Nx} + \\mathbf{b}.\n",
    "\\end{equation}\n",
    "\n",
    "- From this, we get an iteration scheme:\n",
    "    \n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\mathbf{P}\\mathbf{x}^{(k+1)} &= \\mathbf{N}\\mathbf{x}^{(k)} + \\mathbf{b}.\\\\\n",
    "          \\mathbf{x}^{(k+1)} &= \\mathbf{P}^{-1}\\mathbf{N}\\mathbf{x}^{(k)} + \\mathbf{P}^{-1}\\mathbf{b} \\\\\n",
    "                             &= \\mathbf{B}\\mathbf{x}^{(k)} + \\mathbf{f}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "- Another possible scheme is \n",
    "    \n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\mathbf{P}\\mathbf{x}^{(k+1)} &= (\\mathbf{P}-\\mathbf{A})\\mathbf{x}^{(k)} + \\mathbf{b}. \\\\\n",
    "                             &= \\mathbf{P}\\mathbf{x}^{(k)} + (\\mathbf{b}-\\mathbf{A}\\mathbf{x}^{(k)}). \\\\\n",
    "\\therefore \\mathbf{x}^{(k+1)} &= \\mathbf{x}^{(k)} + \\mathbf{P}^{-1}\\mathbf{r}^{(k)},\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "where $\\mathbf{r}^{(k)}$ is the $k$-th **residual vector** (i.e., $\\mathbf{b}-\\mathbf{A}\\mathbf{x}^{(k)}$).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- In the above scheme, a linear system, with coefficient matrix $\\mathbf{P}$, must be solved to update the solution at step $k+1$. \n",
    "- Thus $\\mathbf{P}$, besides being **non-singular** (i.e., invertible), ought to be **easily (i.e., cheaply) invertible** in order to keep the overall computational cost low.\n",
    "- For instace, if $\\mathbf{P}$ were equal to $\\mathbf{A}$ and $\\mathbf{N}=\\mathbf{0}$, the above iteration method would converge in one iteration, but at the same cost of a direct method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Jacobi method\n",
    "- One realization of the above decomposition is $\\mathbf{A} = \\mathbf{D} - (\\mathbf{D}-\\mathbf{A}) = \\mathbf{D} - (\\mathbf{E}+\\mathbf{F})$\n",
    "    - where $\\mathbf{D}$, $\\mathbf{E}$, and $\\mathbf{F}$ are the *diagonal* elements, *negative lower triangular* part and *negative upper triangular* part of $\\mathbf{A}$, respectively.\n",
    "    - The iteration scheme acquired from this decomposition is\n",
    "    \n",
    "    \\begin{equation}\n",
    "    \\mathbf{Dx} = (\\mathbf{E}+\\mathbf{F})\\mathbf{x} + \\mathbf{b}\n",
    "    \\end{equation}\n",
    "    \\begin{equation}\n",
    "    \\mathbf{D}\\mathbf{x}^{(k+1)} = (\\mathbf{E}+\\mathbf{F})\\mathbf{x}^{(k)} + \\mathbf{b}\n",
    "    \\end{equation}\n",
    "    \\begin{align}\n",
    "    \\mathbf{x}^{(k+1)} &= \\mathbf{D}^{-1}\\left[ (\\mathbf{E}+\\mathbf{F})\\mathbf{x}^{(k)} + \\mathbf{b} \\right] \\\\\n",
    "                       &= \\mathbf{D}^{-1}(\\mathbf{E}+\\mathbf{F})\\mathbf{x}^{(k)} + \\mathbf{D}^{-1}\\mathbf{b} \\\\\n",
    "                       &= \\mathbf{B}_{J}\\mathbf{x}^{(k)} + \\mathbf{b}_{J}.\n",
    "    \\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "-    \n",
    "    - This iteration scheme can be written in the following element form:\n",
    "    \n",
    "    \\begin{equation}\n",
    "    x_{i}^{(k+1)} = \\frac{1}{a_{ii}}\\left[ b_{i} - \\sum_{j=1\\\\j\\neq i}^{n} a_{ij}x_{j}^{(k)}  \\right]\\quad i=1,\\cdots,n.\n",
    "    \\end{equation}\n",
    "    - Note that for this method to hold always, $\\mathbf{A}$'s **diagonal elements should be non-zero**.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Gauss-Seidel method\n",
    "- Note that in the Jacobi method, for $i>1$, $x_{i}^{(k+1)}$ are known.\n",
    "- So, it is possible to use them to update the solution:\n",
    "\\begin{equation}\n",
    "    x_{i}^{(k+1)} = \\frac{1}{a_{ii}}\\left[ b_{i} - \\sum_{j=1}^{i-1} a_{ij}x_{j}^{(k+1)} - \\sum_{j=i+1}^{n} a_{ij}x_{j}^{(k)} \\right] \\quad i=1,\\cdots,n\n",
    "\\end{equation}\n",
    "- The decomposition corresponding to the GS is\n",
    "\\begin{equation}\n",
    "\\mathbf{P} = \\mathbf{D} - \\mathbf{E} \\quad \\text{and} \\quad \\mathbf{N} = \\mathbf{F}.\n",
    "\\end{equation}\n",
    "- The associated iteration matrix is\n",
    "\\begin{equation}\n",
    "\\mathbf{B}_{GS} = \\left( \\mathbf{D} - \\mathbf{E} \\right)^{-1} \\mathbf{F}.\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Over-relaxation method\n",
    "- $\\omega$: Relaxation parameter\n",
    "    - $\\omega > 1$: Over-relaxation\n",
    "    - $\\omega < 1$: Under-relaxation\n",
    "    \n",
    "#### Jacobi over-relaxation (JOR) method\n",
    "\\begin{equation}\n",
    "    x_{i}^{(k+1)} = \\frac{\\omega}{a_{ii}}\\left[ b_{i} - \\sum_{j=1\\\\j\\neq i}^{n} a_{ij}x_{j}^{(k)}  \\right] + (1-\\omega)x_{i}^{(k)}.\n",
    "\\end{equation}\n",
    "\n",
    "#### Successive over-relaxation (SOR) method\n",
    "\\begin{equation}\n",
    "    x_{i}^{(k+1)} = \\frac{\\omega}{a_{ii}}\\left[ b_{i} - \\sum_{j=1}^{i-1} a_{ij}x_{j}^{(k+1)} - \\sum_{j=i+1}^{n} a_{ij}x_{j}^{(k)} \\right] + (1-\\omega)x_{i}^{(k)}.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Convergence properties of Jacobi, GS, JOR and SOR\n",
    "\n",
    "- Refer to Sec. 4.2.2 and 4.2.3 of Quarteroni (2000).\n",
    "    - As usual, *nice* (e.g., symmetric and positive definite) matrices are guaranteed to converge.\n",
    "- We apply what we've just learned to the following examples of matrices (Example 4.2 of Quarteroni, 2000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.  0.  4.]\n",
      " [ 7.  4.  2.]\n",
      " [-1.  1.  2.]] \n",
      " [[ 4.  2.  3.]\n",
      " [ 7.  4.  2.]\n",
      " [-1.  1.  2.]] \n",
      " [[-3.  3. -6.]\n",
      " [-4.  7. -8.]\n",
      " [ 5.  7. -9.]] \n",
      " [[ 4.  1.  1.]\n",
      " [ 2. -9.  0.]\n",
      " [ 0. -8. -6.]] \n",
      " [[ 7.  6.  9.]\n",
      " [ 4.  5. -4.]\n",
      " [-7. -3.  8.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as LA\n",
    "\n",
    "A1o = np.array([[ 3.0, 0.0, 4.0],[ 7.0, 4.0, 2.0],[-1.0, 1.0, 2.0]])\n",
    "A1  = np.array([[ 4.0, 2.0, 3.0],[ 7.0, 4.0, 2.0],[-1.0, 1.0, 2.0]])\n",
    "A2  = np.array([[-3.0, 3.0,-6.0],[-4.0, 7.0,-8.0],[ 5.0, 7.0,-9.0]])\n",
    "A3  = np.array([[ 4.0, 1.0, 1.0],[ 2.0,-9.0, 0.0],[ 0.0,-8.0,-6.0]])\n",
    "A4  = np.array([[ 7.0, 6.0, 9.0],[ 4.0, 5.0,-4.0],[-7.0,-3.0, 8.0]])\n",
    "print(A1o,'\\n',A1,'\\n', A2,'\\n', A3,'\\n', A4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def get_convergence_factors(A):\n",
    "    n, m = A.shape\n",
    "    if n != m:\n",
    "        print(\"A is not a square matrix! Aborting.\")\n",
    "        return\n",
    "    D = np.diag(np.diag(A))\n",
    "    E = -(np.tril(A) - D)\n",
    "    F = -(np.triu(A) - D)\n",
    "    #print(A,'\\n',D,'\\n',E,'\\n',F,'\\n',D-(E+F))\n",
    "    #print(LA.inv(D),'\\n',E+F)\n",
    "    BJ = LA.inv(D).dot((E+F))\n",
    "    #print(BJ,LA.eigvals(BJ),np.eye(3)-LA.inv(D).dot(A))\n",
    "    #print(LA.inv(D-E),'\\n',F)\n",
    "    BGS = LA.inv(D-E).dot(F)\n",
    "    #print(BGS,LA.eigvals(BGS))\n",
    "    #print(np.absolute(LA.eigvals(BJ)), np.absolute(LA.eigvals(BGS)))\n",
    "    return [np.max(np.absolute(LA.eigvals(BJ))), np.max(np.absolute(LA.eigvals(BGS)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rho(BG) rho(BGS)\n",
      "[1.1251473563526098, 1.5833333333333335]\n",
      "[1.1086807884578176, 0.40352225657854146]\n",
      "[0.8133091054692768, 1.1111111111111105]\n",
      "[0.4438188250156999, 0.018518518518518517]\n",
      "[0.641132809955697, 0.7745966692414834]\n"
     ]
    }
   ],
   "source": [
    "print('rho(BG)','rho(BGS)')\n",
    "print(get_convergence_factors(A1o))\n",
    "print(get_convergence_factors(A1))\n",
    "print(get_convergence_factors(A2))\n",
    "print(get_convergence_factors(A3))\n",
    "print(get_convergence_factors(A4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gradient Method\n",
    "\n",
    "- We can generalize the idea of relaxation. Consider the following slightly modified form of the iteration scheme:\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\mathbf{x}^{(k+1)} &= \\mathbf{x}^{(k)} + \\mathbf{P}^{-1}\\mathbf{r}^{(k)} \\\\\n",
    "&\\Downarrow \\\\\n",
    "\\mathbf{x}^{(k+1)} &= \\mathbf{x}^{(k)} + \\alpha\\mathbf{P}^{-1}\\mathbf{r}^{(k)},\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "where $\\alpha$ is called **relaxation** or **acceleration parameter**.\n",
    "- Also the iteration scheme is said to be **stationary** if $\\alpha$ is independent of index $k$.\n",
    "    - cf. **Non-stationary method** if $\\mathbf{x}^{(k+1)} = \\mathbf{x}^{(k)} + \\alpha_{k}\\mathbf{P}^{-1}\\mathbf{r}^{(k)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- **Gradient method** is one of the non-stationary method\n",
    "- Note $\\mathbf{Ax}=\\mathbf{b}$'s solution $\\mathbf{x}$ is also the minimizer of\n",
    "\\begin{equation}\n",
    "\\Phi(\\mathbf{y}) = \\frac{1}{2}\\mathbf{y}^{T}\\mathbf{Ay} - \\mathbf{y}^{T}\\mathbf{b},\n",
    "\\end{equation}\n",
    "if $\\mathbf{A}$ is symmetric and positive definite.\n",
    "    - because $\\nabla\\Phi(\\mathbf{y}) = \\frac{1}{2}(\\mathbf{Ay}+\\mathbf{A}^{T}\\mathbf{y}) - \\mathbf{b} = \\mathbf{Ay}-\\mathbf{b}.$\n",
    "    - In other words, a *minimizer* that makes $\\nabla\\Phi=\\mathbf{0}$ is the solution to $\\mathbf{Ay}=\\mathbf{b}$.\n",
    "    - Conversely, if $\\mathbf{x}$ is a solution, then\n",
    "    \\begin{equation}\n",
    "    \\begin{split}\n",
    "      &\\Phi(\\mathbf{y}) = \\Phi(\\mathbf{x} + (\\mathbf{y}-\\mathbf{x})) = \\Phi(\\mathbf{x}) + \\frac{1}{2}(\\mathbf{y}-\\mathbf{x})^{T}\\mathbf{A}(\\mathbf{y}-\\mathbf{x}) \\\\\n",
    "      &\\therefore \\Phi(\\mathbf{y}) > \\Phi(\\mathbf{x})\\text{ for } ^{\\forall}\\mathbf{y} \\neq \\mathbf{x}. \\\\\n",
    "      &\\therefore \\mathbf{x} \\text{ is a minimizer}.\n",
    "    \\end{split}\n",
    "    \\end{equation}\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- How to find $\\mathbf{x}$ starting from an initial guess, $\\mathbf{x}^{(0)}$?\n",
    "- In other words, the goal is to find $\\mathbf{x}$ such that\n",
    "\\begin{equation}\n",
    "\\mathbf{x} = \\mathbf{x}^{(0)} + \\alpha \\mathbf{d}.\n",
    "\\end{equation}\n",
    "- Of course, $\\alpha$ and $\\mathbf{d}$ are not known *a priori*.\n",
    "- So, we take steps:\n",
    "\\begin{equation}\n",
    "\\mathbf{x}^{(k+1)} = \\mathbf{x}^{(k)} + \\alpha_{k} \\mathbf{d}^{(k)}\\quad k=0,1,2,\\cdots.\n",
    "\\end{equation}\n",
    "- One natural idea is to take the descent direction of maximum slope: i.e., $\\mathbf{d} = -\\nabla \\Phi(\\mathbf{x}^{(k)})$.\n",
    "    - For this reason, the method is called **gradient method** or **steepest descent method**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- We saw $\\nabla \\Phi(\\mathbf{x}^{(k)}) = \\mathbf{A}\\mathbf{x}^{(k)} - \\mathbf{b} = -\\mathbf{r}^{(k)}$.\n",
    "    - $\\therefore$ in the GM, $\\mathbf{x}^{(k)}$ **moves along the direction** $\\mathbf{r}^{(k)}$ to get to $\\mathbf{x}^{(k+1)}$.\n",
    "    \n",
    "- Since we've found the direction, we now need to know how far to move along that direction, which is quantified by $\\alpha_{k}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- To find $\\alpha_{k}$, note that\n",
    "\\begin{equation}\n",
    "\\Phi(\\mathbf{x}^{(k+1)}) = \\frac{1}{2} \\left( \\mathbf{x}^{(k)}+\\alpha \\mathbf{r}^{(k)} \\right)^{T}\\mathbf{A} \\left( \\mathbf{x}^{(k)}+\\alpha \\mathbf{r}^{(k)} \\right) - \\left(\\mathbf{x}^{(k)}+\\alpha \\mathbf{r}^{(k)} \\right)^{T}\\mathbf{b}.\n",
    "\\end{equation}\n",
    "\n",
    "    - Take the partical derivative of the above expression of $\\Phi$ with respect to $\\alpha$.\n",
    "    - $\\alpha$ that make that derivative equal to zero will provide the local minimum of $\\Phi$ along the direction $\\mathbf{d}$.\n",
    "    \\begin{equation}\n",
    "    \\begin{split}\n",
    "    \\frac{\\partial\\Phi}{\\partial\\alpha_{k}} &= \\frac{1}{2} \\mathbf{r}^{(k)T}\\mathbf{A} \\left( \\mathbf{x}^{(k)}+\\alpha_{k} \\mathbf{r}^{(k)} \\right) + \\frac{1}{2} \\left( \\mathbf{x}^{(k)}+\\alpha_{k} \\mathbf{r}^{(k)} \\right)^{T}\\mathbf{A} \\mathbf{r}^{(k)} - \\mathbf{r}^{(k)T}\\mathbf{b} \\\\\n",
    "    &= \\mathbf{r}^{(k)T} \\mathbf{A} \\left( \\mathbf{x}^{(k)}+\\alpha_{k} \\mathbf{r}^{(k)} \\right)  - \\mathbf{r}^{(k)T}\\mathbf{b} \\\\\n",
    "    &= \\mathbf{r}^{(k)T} \\left( \\mathbf{A}\\mathbf{x}^{(k)}-\\mathbf{b} \\right) + \\alpha_{k} \\left( \\mathbf{r}^{(k)T}\\mathbf{A}\\mathbf{r}^{(k)} \\right) \\\\\n",
    "    &= -\\mathbf{r}^{(k)T}\\mathbf{r}^{(k)} + \\alpha_{k} \\left( \\mathbf{r}^{(k)T}\\mathbf{A}\\mathbf{r}^{(k)} \\right) \\\\\n",
    "    &= 0.\n",
    "    \\end{split}\n",
    "    \\end{equation}\n",
    "    \\begin{equation}\n",
    "    \\therefore \\alpha_{k} = \\frac{\\mathbf{r}^{(k)T}\\mathbf{r}^{(k)}}{ \\mathbf{r}^{(k)T}\\mathbf{A}\\mathbf{r}^{(k)} }\n",
    "    \\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- In summary, in GM, given $\\mathbf{x}^{(0)}$, for $k=0,1,\\cdots$, until convergence, compute\n",
    "\\begin{align}\n",
    "\\mathbf{r}^{(k)} &= \\mathbf{b} - \\mathbf{A}\\mathbf{x}^{(k)} \\\\\n",
    "\\alpha_{k} &= \\frac{\\mathbf{r}^{(k)T}\\mathbf{r}^{(k)}}{ \\mathbf{r}^{(k)T}\\mathbf{A}\\mathbf{r}^{(k)} } \\\\\n",
    "\\mathbf{x}^{(k+1)} &= \\mathbf{x}^{(k)} + \\alpha_{k} \\mathbf{r}^{(k)}.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Let's try to have some geometric intuition of the GM.\n",
    "    - When $\\Phi$ is such that the contours are concentric circles\n",
    "        - 1 step is sufficient.\n",
    "    - When the countours are highly elongated ellipses,\n",
    "        - Convergence can be very slow.\n",
    "    \n",
    "    \n",
    "- GM consists of 2 operations:\n",
    "    1. Finding search direction (steepest descent)\n",
    "    2. Picking up a point of local minimum along that direction.\n",
    "    \n",
    "    - However, the two operations are **independent**. \n",
    "    - This realization is the starting point of the **conjugate gradient method**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conjugate Gradient Method\n",
    "\n",
    "- CG and GM has the same starting point\n",
    "    - The solution to $\\mathbf{Ax}=\\mathbf{b}$ is also the minimizer of $\\Phi(\\mathbf{y}) = \\frac{1}{2}\\mathbf{y}^{T}\\mathbf{Ay} - \\mathbf{y}^{T}\\mathbf{b}$ when $\\mathbf{A}$ is summetric and positive definite.\n",
    "- The key difference between CG and GM is\n",
    "    - GM follows **the steepest descent directions**\n",
    "    - CG follows **mutually A-orthogonal directions**\n",
    "    - Two vectors $x$ and $y$ are **conjugate** or **A-orthogonal** if $\\mathbf{x}^{T}\\mathbf{Ay}=\\mathbf{0}$ for a symmetric non-singular matrix.\n",
    "\n",
    "- Our task is to understand why it is better to follow mutually A-orthogonal directions. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's recall that, for a given *search* direction $\\mathbf{p}^{(k)}$, finding how far to move along that direction is straightforward:\n",
    "If $\\mathbf{x}^{(k+1)} = \\mathbf{x}^{(k)} + \\alpha_{k}\\mathbf{p}^{(k)}$,\n",
    "\n",
    "\\begin{equation}\n",
    "\\Phi\\left(\\mathbf{x}^{(k+1)}\\right) = \\frac{1}{2}\\left(\\mathbf{x}^{(k)} + \\alpha_{k}\\mathbf{p}^{(k)} \\right)^{T}\\mathbf{A}\\left(\\mathbf{x}^{(k)} + \\alpha_{k}\\mathbf{p}^{(k)} \\right) - \\left(\\mathbf{x}^{(k)} + \\alpha_{k}\\mathbf{p}^{(k)} \\right)^{T}\\mathbf{b}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial\\Phi}{\\partial \\alpha_{k}} = \\frac{1}{2}\\mathbf{p}^{(k)T}\\mathbf{A}\\left(\\mathbf{x}^{(k)}+ \\alpha_{k}\\mathbf{p}^{(k)} \\right) \n",
    "+ \\frac{1}{2}\\left(\\mathbf{x}^{(k)} + \\alpha_{k}\\mathbf{p}^{(k)} \\right)^{T}\\mathbf{A}\\mathbf{p}^{(k)} - \\mathbf{p}^{(k)T}\\mathbf{b} = 0\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\left( \\mathbf{p}^{(k)T}\\mathbf{A}\\mathbf{p}^{(k)} \\right)\\alpha_{k} = \\mathbf{p}^{(k)T}\\left( \\mathbf{b}-\\mathbf{A}\\mathbf{x}^{(k)} \\right) = \\mathbf{p}^{(k)T}\\mathbf{r}^{(k)}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\therefore \\alpha_{k} = \\frac{\\mathbf{p}^{(k)T}\\mathbf{r}^{(k)}}{\\mathbf{p}^{(k)T}\\mathbf{A}\\mathbf{p}^{(k)}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- The remaining question is how to find a suitable $\\mathbf{p}^{(k)}$.\n",
    "    - When $\\mathbf{p}^{(k)} = \\mathbf{r}^{(k)}$, we get GM.\n",
    "- To be able to do so, we need to first define what *suitable* means.\n",
    "- **Definition**: $\\mathbf{x}^{(k)}$ is said to be **optimal** with respect to a certain direction $\\mathbf{p}\\neq\\mathbf{0}$ if\n",
    "\\begin{equation}\n",
    "\\Phi\\left(\\mathbf{x}^{(k)} \\right) \\le \\Phi\\left(\\mathbf{x}^{(k)}+\\lambda\\mathbf{p}\\right)\\quad ^{\\forall}\\lambda \\in \\mathbb{R}.\n",
    "\\end{equation}\n",
    "- Note that the optimal $\\mathbf{x}^{(k)}$ is the local minimizer of $\\Phi$ along $\\mathbf{p}$ requiring $\\lambda=0$: i.e., no need to move because we are already at the local minimizer.\n",
    "- This realization leads to the following statement:\n",
    "\\begin{equation}\n",
    "\\frac{\\partial\\Phi}{\\partial \\lambda} = \\mathbf{p}^{T}\\left( \\mathbf{A}\\mathbf{x}^{(k)} - \\mathbf{b} \\right) + \\lambda \\mathbf{p}^{T}\\mathbf{A}\\mathbf{p} = -\\mathbf{p}^{T}\\mathbf{r}^{(k)} + \\lambda \\mathbf{p}^{T}\\mathbf{A}\\mathbf{p}\n",
    "\\end{equation}\n",
    "    - Since $\\partial \\Phi/\\partial \\lambda$ should be 0 when $\\lambda = 0$, $\\mathbf{p}^{T}\\mathbf{r}^{(k)} = 0$ when $\\mathbf{x}^{(k)}$ is optimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We have the following general relationship between the current and the next residual:\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\mathbf{r}^{(k+1)} &= \\mathbf{b} - \\mathbf{A}\\mathbf{X}^{(k+1)} \\\\\n",
    "                   &= \\mathbf{b} - \\mathbf{A}\\left( \\mathbf{X}^{(k)} + \\alpha_{k}\\mathbf{p}^{(k)} \\right) \\\\\n",
    "                   &= \\mathbf{r}^{(k)} -  \\alpha_{k}\\mathbf{A}\\mathbf{p}^{(k)}.\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Since $\\mathbf{p}^{(k)} = \\mathbf{r}^{(k)}$ in GM,\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\mathbf{r}^{(k)T}\\mathbf{r}^{(k+1)} &= \\mathbf{r}^{(k)T} \\left( \\mathbf{r}^{(k)} -  \\alpha_{k}\\mathbf{A}\\mathbf{r}^{(k)} \\right) \\\\\n",
    "                                    &= \\mathbf{r}^{(k)T}\\mathbf{r}^{(k)} - \\alpha_{k}\\mathbf{r}^{(k)T}\\mathbf{A}\\mathbf{r}^{(k)} = 0\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "The above equation holds always because of the definition of $\\alpha_{k}$.\n",
    "So, in GM, $\\mathbf{r}^{(k)} \\perp \\mathbf{r}^{(k+1)}$ but in general, $\\mathbf{r}^{(k)} \\not\\perp \\mathbf{r}^{(k+2)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- The next important question is this: **Are there descent directions that maintain the optimality of iterates?**\n",
    "    - The answer is **YES**. \n",
    "\n",
    "- To fomulate the way of finding them, let's go back to the iteration scheme: $\\mathbf{x}^{(k+1)} = \\mathbf{x}^{(k)} + \\mathbf{q}$.\n",
    "    - Let's also assume that $\\mathbf{x}^{(k)}$ is optimal with respect to a direction $\\mathbf{p}$ (i.e., $\\mathbf{r}^{(k)}\\perp \\mathbf{p}$).\n",
    "    - We are going to require $\\mathbf{x}^{(k+1)}$ is still optimal w.r.t. $\\mathbf{p}$: i.e., $\\mathbf{r}^{(k+1)}\\perp \\mathbf{p}$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "0 &= \\mathbf{p}^{T}\\mathbf{r}^{(k+1)} = \\mathbf{p}^{T} \\left( \\mathbf{b} - \\mathbf{A}\\mathbf{x}^{(k+1)} \\right) \\\\\n",
    "  &= \\mathbf{p}^{T} \\left( \\mathbf{b} - \\mathbf{A}\\left(\\mathbf{x}^{(k)}+\\mathbf{q}\\right) \\right)\n",
    "  &= \\mathbf{p}^{T}\\mathbf{r}^{(k)} - \\mathbf{p}^{T}\\mathbf{A}\\mathbf{q}.\\\\\n",
    "  &\\therefore \\mathbf{p}^{T}\\mathbf{A}\\mathbf{q} = 0.\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "- The conclusion is that directions $\\mathbf{p}$ and $\\mathbf{q}$ should be **A-conjugate** in order to maintain the optimality of $\\mathbf{x}^{(k+1)}$ w.r.t. $\\mathbf{p}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To find a sequence of pairwise A-conjugate directions, we set\n",
    "\\begin{equation}\n",
    "\\mathbf{p}^{(k+1)} = \\mathbf{r}^{(k+1)} - \\beta_{k}\\mathbf{p}^{(k)}\\quad k=0,1,\\cdots.\n",
    "\\end{equation}\n",
    "\n",
    "The conjugacy condition gives\n",
    "\\begin{equation}\n",
    "\\mathbf{p}^{(k)T}\\mathbf{A}\\mathbf{p}^{(k+1)} = \\mathbf{p}^{(k)T}\\mathbf{A}\\left(\\mathbf{r}^{(k+1)} - \\beta_{k}\\mathbf{p}^{(k)} \\right) = 0.\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\therefore \\beta_{k} = \\frac{\\mathbf{p}^{(k)T}\\mathbf{A}\\mathbf{r}^{(k+1)}}{\\mathbf{p}^{(k)T}\\mathbf{A}\\mathbf{p}^{(k)}}.\n",
    "\\end{equation}\n",
    "\n",
    "Note that $\\mathbf{p}^{(j)T}\\mathbf{A}\\mathbf{p}^{(k+1)}=0$ for $j=0,1,\\cdots,k$ as we require the descent directions to be.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In summary, for $k=0, 1, \\cdots$,\n",
    "\\begin{align}\n",
    "\\mathbf{r}^{(0)} &= \\mathbf{b} - \\mathbf{A}\\mathbf{x}^{(0)},\\ \\mathbf{p}^{(0)}=\\mathbf{r}^{(0)} \\\\\n",
    "\\alpha_{k} &= \\frac{\\mathbf{p}^{(k)T}\\mathbf{r}^{(k)}}{\\mathbf{p}^{(k)T}\\mathbf{A}\\mathbf{p}^{(k)}} \\\\\n",
    "\\mathbf{x}^{(k+1)} &= \\mathbf{x}^{(k)} + \\alpha_{k} \\mathbf{p}^{(k)} \\\\\n",
    "\\mathbf{r}^{(k+1)} &= \\mathbf{r}^{(k)} - \\alpha_{k} \\mathbf{A} \\mathbf{p}^{(k)} \\\\\n",
    "\\beta_{k} &= \\frac{\\mathbf{p}^{(k)T}\\mathbf{A}\\mathbf{r}^{(k+1)}}{\\mathbf{p}^{(k)T}\\mathbf{A}\\mathbf{p}^{(k)}} \\\\\n",
    "\\mathbf{p}^{(k+1)} &= \\mathbf{r}^{(k+1)} - \\beta_{k}\\mathbf{p}^{(k)}.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Unlike the classical iterative methods, CG converges to the exact solution within $n$ steps if $\\mathbf{A}$ is $n\\times n$!\n",
    "    - Not so useful for large $n$ though.\n",
    "- Preconditioning is very important for large and complicated problems.\n",
    "- Multi-grid method: See Ismael-Zadeh and Tackley\n",
    "    - Superb complexity that is almost linear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Brief introduction to Krylov subspace methods\n",
    "\n",
    "Let's recall the interation scheme:\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\mathbf{x}^{(k+1)} &= \\mathbf{x}^{(k)} + \\alpha_{k}\\mathbf{P}^{-1}\\mathbf{r}^{(k)} \\\\\n",
    "                   &= \\mathbf{x}^{(k)} + \\alpha_{k}\\mathbf{P}^{-1}\\left(\\mathbf{b}-\\mathbf{A}\\mathbf{x}^{(k)} \\right) \\\\\n",
    "                   &= \\underbrace{\\left( \\mathbf{I} - \\alpha_{k}\\mathbf{P}^{-1}\\mathbf{A}\\right)}_{\\text{Iteration matrix}}\\mathbf{x}^{(k)} + \\alpha_{k}\\mathbf{P}^{-1}\\mathbf{b}.\n",
    "\\end{split}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can show that if $\\mathbf{P}=\\mathbf{I}$, \n",
    "\\begin{equation}\n",
    " \\mathbf{r}^{(k)} = \\left( \\prod_{j=0}^{k-1}\\left( \\mathbf{I} - \\alpha_{j}\\mathbf{A} \\right) \\right) \\mathbf{r}^{(0)}.\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "  \\mathbf{r}^{(1)} &= \\mathbf{b} - \\mathbf{A}\\mathbf{x}^{(1)} \\\\\n",
    "                   &= \\mathbf{b} - \\mathbf{A}\\left( \\left( \\mathbf{I} - \\alpha_{0}\\mathbf{A} \\right) \\mathbf{x}^{(0)} + \\alpha_{0}\\mathbf{b} \\right) \\\\\n",
    "                   &= \\left( \\mathbf{b} - \\mathbf{A}\\mathbf{x}^{(0)} \\right) - \\alpha_{0}\\mathbf{A}\\left( \\mathbf{b} - \\mathbf{A}\\mathbf{x}^{(0)} \\right) \\\\\n",
    "                   &= \\left( \\mathbf{I} - \\alpha_{0}\\mathbf{A} \\right) \\mathbf{r}^{(0)}.\n",
    "\\end{split}  \n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "  \\mathbf{r}^{(2)} &= \\mathbf{b} - \\mathbf{A}\\mathbf{x}^{(2)} \\\\\n",
    "                   &= \\cdots \\\\\n",
    "                   &= \\left( \\mathbf{I} - \\alpha_{0}\\mathbf{A} \\right)\\left( \\mathbf{I} - \\alpha_{1}\\mathbf{A} \\right) \\mathbf{r}^{(0)}.\n",
    "\\end{split}  \n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "  &\\vdots \\\\\n",
    "  &\\mathbf{r}^{(k)} = p_{k}(\\mathbf{A}) \\mathbf{r}^{(0)}.\n",
    "\\end{split}  \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Define **Krylov subspace of order $m$** as \n",
    "\\begin{equation}\n",
    "\\mathcal{K}_{m}(\\mathbf{A};\\mathbf{v}) = \\text{span}\\left\\{\\mathbf{v}, \\mathbf{Av}, \\mathbf{A}^{2}\\mathbf{v}, \\cdots,  \\mathbf{A}^{m-1}\\mathbf{v} \\right\\}\n",
    "\\end{equation}\n",
    "\n",
    "Then, $\\mathbf{r}^{(k)} \\in \\mathcal{K}_{k+1}(\\mathbf{A};\\mathbf{r}^{(0)})$.\n",
    "\n",
    "Again if $\\mathbf{P}=\\mathbf{I}$, $\\mathbf{x}^{(k+1)} = \\mathbf{x}^{(k)} + \\alpha_{k}\\mathbf{r}^{(k)}$. So, we get\n",
    "\\begin{equation}\n",
    "\\mathbf{x}^{(k)} = \\mathbf{x}^{(0)} + \\sum_{j=0}^{k-1}\\alpha_{j}\\mathbf{r}^{(j)}.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's define yet another space\n",
    "\\begin{equation}\n",
    "  W_{k} = \\left\\{\\mathbf{v} \\vert \\mathbf{v} = \\mathbf{x}^{(0)} + \\mathbf{y}, \\mathbf{y} \\in \\mathcal{K}_{k}(\\mathbf{A}; \\mathbf{r}^{(0)}) \\right\\}.\n",
    "\\end{equation}\n",
    "\n",
    "A Krylov subspace method looks for an approximate solution of the form\n",
    "\\begin{equation}\n",
    "  \\mathbf{x}^{(k)} = \\mathbf{x}^{(0)} + q_{k-1}(\\mathbf{A})\\mathbf{r}^{(0)} \\in W_{k}.\n",
    "\\end{equation}\n",
    "\n",
    "Let's try to get a little more concrete understanding of this class of methods by realizing that CGM can be considered as a Kyrlov subspace method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Recall the CGM algorithm: For $k=0, 1, \\cdots$,\n",
    "\\begin{equation}\n",
    "\\mathbf{x}^{(k+1)} = \\mathbf{x}^{(k)} + \\alpha_{k} \\mathbf{p}^{(k)}.\n",
    "\\end{equation}\n",
    "\n",
    "So, \n",
    "\\begin{align}\n",
    "\\mathbf{x}^{(1)} &= \\mathbf{x}^{(0)} + \\alpha_{0} \\mathbf{p}^{(0)} \\\\\n",
    "\\mathbf{x}^{(2)} &= \\mathbf{x}^{(1)} + \\alpha_{0} \\mathbf{p}^{(1)} \\\\\n",
    "                 &= \\mathbf{x}^{(0)} + \\alpha_{0} \\mathbf{p}^{(0)} + \\alpha_{0} \\mathbf{p}^{(1)} \\\\\n",
    "                 &\\vdots \\\\\n",
    "\\mathbf{x}^{(k)} &= \\mathbf{x}^{(0)} + \\sum_{j=0}^{k-1}\\alpha_{j}\\mathbf{p}^{(j)}.\n",
    "\\end{align}\n",
    "\n",
    "To see the equivalence, we have to determine that\n",
    "\\begin{equation}\n",
    " \\sum_{j=0}^{k-1}\\alpha_{j}\\mathbf{p}^{(j)} = q_{k-1}(\\mathbf{A})\\mathbf{r}^{(0)}.\n",
    "\\end{equation}\n",
    "In other words, is the following true?\n",
    "\\begin{equation}\n",
    " \\text{span}\\left\\{ \\mathbf{p}^{(0)}, \\mathbf{p}^{(1)}, \\ldots, \\mathbf{p}^{(k-1)} \\right\\} = \\mathcal{K}_{k}(\\mathbf{A};\\mathbf{r}^{(0)}).\n",
    "\\end{equation}\n",
    "The answer is YES."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "According to the CGM algorithm,\n",
    "\\begin{align}\n",
    "\\mathbf{x}^{(1)} &= \\mathbf{x}^{(0)} + \\alpha_{0}\\mathbf{p}^{(0)} \\\\\n",
    "\\mathbf{r}^{(1)} &= \\mathbf{r}^{(0)} - \\alpha_{0} \\mathbf{A} \\mathbf{p}^{(0)} \\\\\n",
    "\\mathbf{p}^{(1)} &= \\mathbf{r}^{(1)} - \\beta_{0}\\mathbf{p}^{(0)} \\\\\n",
    "                 &= \\mathbf{r}^{(0)} - \\alpha_{0} \\mathbf{A} \\mathbf{p}^{(0)} - \\beta_{0}\\mathbf{p}^{(0)} \\\\\n",
    "                 &= \\left[(1-\\beta_{0})\\mathbf{I}-\\alpha_{0}\\mathbf{A}\\right]\\mathbf{r}^{(0)} \\\\\n",
    "                 & \\\\\n",
    "\\mathbf{x}^{(2)} &= \\mathbf{x}^{(0)} + \\alpha_{0}\\mathbf{p}^{(0)} + \\alpha_{1}\\mathbf{p}^{(1)} \\\\\n",
    "\\mathbf{r}^{(2)} &= \\mathbf{r}^{(1)} - \\alpha_{1} \\mathbf{A} \\mathbf{p}^{(1)} \\\\\n",
    "                 &= \\left( \\mathbf{r}^{(0)} - \\alpha_{0} \\mathbf{A} \\mathbf{p}^{(0)} \\right) - \\alpha_{1} \\mathbf{A} \\left[(1-\\beta_{0})\\mathbf{I}-\\alpha_{0}\\mathbf{A}\\right]\\mathbf{r}^{(0)} \\\\\n",
    "\\mathbf{p}^{(2)} &= \\mathbf{r}^{(2)} - \\beta_{1}\\mathbf{p}^{(1)} \\\\\n",
    "                 &= q_{2}(\\mathbf{A})\\mathbf{r}^{(0)} \\\\\n",
    "                 &\\vdots\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- The basis for $\\mathcal{K}_{m}(\\mathbf{A};\\mathbf{r}^{(0)})$ are not a well-conditioned one.\n",
    "- An equivalent orthonormal basis is used instead (e.g., the Arnoldi algorithm).\n",
    "- The practical question is how to find a suitable linear combination of the bases.\n",
    "    1. Full Orthogonalization Method (FOM): compute $\\mathbf{x}^{(k)} \\in W_{k}$ such that\n",
    "    \\begin{equation}\n",
    "      \\mathbf{v}^{T}(\\mathbf{b}-\\mathbf{A}\\mathbf{x}^{(k)}) = 0\\ \\text{for}\\ ^{\\forall}\\mathbf{v} \\in \\mathcal{K}_{k}(\\mathbf{A};\\mathbf{r}^{(0)}).\n",
    "    \\end{equation}\n",
    "        - CGM is an FOM!\n",
    "    2. Generalized Minimum Residual Method (GMRES): compute $\\mathbf{x}^{(k)} \\in W_{k}$ minimizing the Euclidean norm of $\\mathbf{r}^{(k)}$:\n",
    "    \\begin{equation}\n",
    "      \\left\\lVert \\mathbf{b}-\\mathbf{A}\\mathbf{x}^{(k)} \\right\\rVert_{2} = \\min_{\\mathbf{v}\\in W_{k}} \\left\\lVert \\mathbf{b}-\\mathbf{A}\\mathbf{v} \\right\\rVert_{2}.\n",
    "    \\end{equation}\n",
    "        - It takes a little more effort to understand how this criterion leads to a programmable iteration algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Krylov subspace methods are known for their efficienty and flexibility in solving large sparse system. \n",
    "    - Even for non-symmetric $\\mathbf{A}$.\n",
    "- For more, also see \"Numerical Linear Algebra with Applications : Using MATLAB\" by W. Ford (First edition. London : Academic Press. 2015).\n",
    "    - The university library has an ebook version: http://ezproxy.memphis.edu/login?url=http://search.ebscohost.com/login.aspx?direct=true&db=e000xna&AN=485990&site=ehost-live&ebv=EB&ppid=pp_146"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
