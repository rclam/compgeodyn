{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Principles of Numerical Mathematics\n",
    "\n",
    "Mostly from Ch. 2 of Quarteroni (2000)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Well-posedness and condition number\n",
    "\n",
    "### Definition of problem types\n",
    "A problem is to find $x$ suth that $F(x,d)=0$, where $d$ is a set of data and $F$ is the functional relationship between $x$ and $d$.\n",
    "- Direct  : $x$ unknown, $F$ and $d$ given\n",
    "- Inverse : $d$ unknown, $F$ and $x$ given\n",
    "- Identification : $F$ unknown, $x$ and $d$ given\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Well-posedness and condition number\n",
    "\n",
    "### Well-posed problem\n",
    "A (PDE) problem is called *well posed* or *stable* provided it has a **unique** solution which depends **continuously** on the given data. Otherwise, it is called *ill-posed* or *improperly posed* or *unstable.*\n",
    "#### Example\n",
    "\\begin{equation}\n",
    "p(x) = x^{4}-x^{2}(2a-1)+a(a-1) \n",
    "\\end{equation}\n",
    "has discontinuous variation of the number of real roots: 4 if $a \\ge 1$, 2 if $a \\in [0,1)$ and 0 if $a \\lt 0$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Well-posedness and condition number\n",
    "\n",
    "### Well-posed problem\n",
    "\n",
    "#### Meaning of *continuous dependence* \n",
    "Continuous dependence on the data means that **small** perturbations on the data $d$ yield **small** changes in the solution $x$.\n",
    "- $\\delta d$: An admissiable perturbation on the data\n",
    "- $\\delta x$: The consequent change in the solution\n",
    "- $F(x+\\delta x, d + \\delta d) = 0$, then,\n",
    "\\begin{equation}\n",
    "^{\\forall}\\eta > 0, \\  ^{\\exists}K(\\eta,d) \\text{ such that } \\Vert \\delta d \\Vert < \\eta \\Rightarrow \\Vert \\delta x \\Vert \\le K(\\eta, d) \\Vert \\delta d \\Vert.\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Well-posedness and condition number\n",
    "\n",
    "### Well-posed problem\n",
    "\n",
    "#### Condition number\n",
    "- Relative condition number\n",
    "\\begin{equation}\n",
    " K(d) = \\sup_{\\delta d \\in D} \\frac{ \\Vert \\delta x \\Vert / \\Vert x \\Vert}{\\Vert \\delta d \\Vert / \\Vert d \\Vert},\n",
    "\\end{equation}\n",
    "where $D$ is a small neighborhood around the origin for *admissable* perturbations.\n",
    "- Absolute condition number for $x=0$ or $d=0$\n",
    "\\begin{equation}\n",
    " K(d) = \\sup_{\\delta d \\in D} \\frac{\\Vert \\delta x \\Vert}{\\Vert \\delta d \\Vert}.\n",
    "\\end{equation}\n",
    "\n",
    "- *ill-conditioned* if $K(d)$ is *big*, of which precise meaning varies from one problem to another.\n",
    "- *ill-conditioned* $\\neq$ *ill-posed*! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### Example 1\n",
    "\\begin{equation}\n",
    "x^{2} - 2px + 1 = 0 \\quad (p \\ge 1)\n",
    "\\end{equation}\n",
    "The solution is $x_{\\pm}=p \\pm \\sqrt{p^{2}-1}$. \n",
    "Now, let a function $F(x,p) = x^{2}-2px +1$, where the \"datum\" is the coefficient $p$, and $x$ is the unknown vector of components $\\{x_{+},x_{-}\\}$.\n",
    "\n",
    "Let's also define a *resolvent* $G: x=G(p)$, where $x$ is the solution, such that $F(G(p),p)=0$.\n",
    "Expressing the solutions using resolvents, we get\n",
    "\\begin{equation}\n",
    "  x_{\\pm} = G_{\\pm}(p) = p \\pm \\sqrt{p^{2}-1}.\n",
    "\\end{equation}\n",
    "and \n",
    "\\begin{equation}\n",
    "  G^{\\prime}_{\\pm}(p) = 1 \\pm \\frac{p}{\\sqrt{p^{2}-1}}.\n",
    "\\end{equation}\n",
    "\n",
    "From\n",
    "\\begin{equation}\n",
    "x + \\delta x = G(p+\\delta p),\n",
    "\\end{equation}\n",
    "we get\n",
    "\\begin{equation}\n",
    "\\delta x = G(p+\\delta p)-x = G(p+\\delta p)-G(p).\n",
    "\\end{equation}\n",
    "\n",
    "The Taylor's expansion of $G$ tells us that\n",
    "\\begin{equation}\n",
    " G(p+\\delta p)-G(p) = G^{\\prime}(p)\\delta p + O(||\\delta p||) \\quad \\text{for} \\quad \\delta p \\rightarrow 0.\n",
    "\\end{equation}\n",
    "\n",
    "Thus, \n",
    "\\begin{equation}\n",
    "\\delta x \\approx G^{\\prime}(p)\\delta p.\n",
    "\\end{equation}\n",
    "\n",
    "Now we get the following form of the condition number:\n",
    "\\begin{equation}\n",
    "K(p) = \\sup_{\\delta p \\in D} \\frac{\\Vert \\delta x \\Vert / \\Vert x \\Vert}{\\Vert \\delta p \\Vert / \\Vert p \\Vert } = \\sup_{\\delta p \\in D} \\frac{ \\Vert G^{\\prime}(p)\\delta p \\Vert  \\,  \\Vert p \\Vert }{ \\Vert G(p) \\Vert  \\,  \\Vert \\delta p \\Vert }\n",
    " \\approx  \\Vert G^{\\prime}(p) \\Vert  \\frac{ \\Vert p \\Vert }{ \\Vert G(p) \\Vert }\n",
    "\\end{equation}\n",
    "\n",
    "Finally, we get\n",
    "\\begin{equation}\n",
    "K(p) = \\left\\Vert 1 \\pm \\frac{p}{\\sqrt{p^{2}-1}}  \\right\\Vert \\frac{\\Vert p\\Vert }{p \\pm \\sqrt{p^{2}-1}} = \\frac{|p|}{\\sqrt{p^{2}-1}}\n",
    "\\end{equation}\n",
    "\n",
    "So, $K$ is small (.ie., $\\sim$1) for $p \\ge \\sqrt{2}$; goes to $\\infty$ as $p \\rightarrow 1$ making the problem (ill-posed or ill-conditioned?).\n",
    "\n",
    "However, this problem can **regularized**: i.e., the singularity at $p=1$ can be removed by the change of parameters.\n",
    "By letting $t = p + \\sqrt{p^{2}-1}$ and $F(x,t) = x^{2} - ((1+t^{2})/t)x + 1 = 0$, the roots become $x=t$ and $1/t$ and coincide with each other when $t=1$ (i.e., $p=1$).\n",
    "\n",
    "#### Example 2\n",
    "Let's consider a linear mapping:\n",
    "\\begin{equation}\n",
    " f: \\mathbb{R}^{2} \\rightarrow \\mathbb{R}, \\quad f(a,b) = a+b\n",
    "\\end{equation}\n",
    "\n",
    "Let's note that the resolvent $G$ is equal to $f(a,b)$ in this case and the gradient is the vector $f^{\\prime}(a,b)=(1,1)$. Also, the data ($d$) in this problem is a vector composed of two parameters, $(a,b)$.\n",
    "Using $L_{1}$ norm ($\\Vert \\mathbf{a} \\Vert_{1}=\\sum_{i=1}^{n}|a_{i}|$), we get the condition number:\n",
    "\\begin{equation}\n",
    "  K(a,b) \\approx \\Vert G^{\\prime}(d) \\Vert_{1}  \\frac{ \\Vert d \\Vert_{1} }{ \\Vert G(d) \\Vert_{1} } = 2\\frac{|a|+|b|}{|a+b|}\n",
    "\\end{equation}\n",
    "\n",
    "If $a$, $b$ are of the same sign, $K=1$ and the problem is will-posed;\n",
    "if $a$, $b$ are almost equal but of the opposite signs, $K \\rightarrow \\infty$ and the problem is ill-conditioned.\n",
    "The ill-conditioned situation arises from the **cancellation of significant digits**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stability of Numerical Methods\n",
    "\n",
    "A numerical method for the approximate solution of $F(x,d)=0$ will consist, in general, of a sequence of approximate problems\n",
    "\\begin{equation}\n",
    "F_{n}(x_{n}, d_{n})=0 \\quad n \\ge 1\n",
    "\\end{equation}\n",
    "such that $x_{n} \\rightarrow x$ as $n \\rightarrow \\infty$: In other words, **the numerical solution converges to the exact solution**. A typical example of data $d_{n}$ is grid spacing (often denoted as $h$) that are sequentially refined. \n",
    "\n",
    "In methods for finding an approximate solution to a partial differential equation such as finite difference method or finite element method, it is a common practice to prove convergence by showing that error or residual decreases at the expected rate as $h$ gets smaller.\n",
    "\n",
    "[//]: # \"Is such a demonstration sufficient to show that the employed numerical method is convergent? Most of the time but not always. That's why we distinguish convergence from **consistency**:\n",
    "The problem sequence is **consistent** if $F_{n}(x,d) - F(x,d) \\rightarrow 0$ for $n \\rightarrow \\infty$.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Stability of Numerical Methods\n",
    "\n",
    "For a numerical method to be **stable** or **well-posed**, we require for any fixed $n$,\n",
    "\n",
    "1. there exists a unique solution $x_{n}$ corresponding to $d_{n}$\n",
    "2. $x_{n}$ is a unique and continuous function of $d_{n}$:\n",
    "\\begin{equation}\n",
    "^{\\forall}\\eta > 0, ^{\\exists}K_{n}(\\eta, d_{n}): \\Vert \\delta d_{n} \\Vert < \\eta \\Rightarrow \\Vert \\delta x_{n} \\Vert \\le K_{n} \\Vert \\delta d_{n} \\Vert,\n",
    "\\end{equation}\n",
    "where the condition number $K_{n}$ should be understood as a relative or an absolute one according to the context. Recall the following definitions:\n",
    "\\begin{equation}\n",
    " K_{n}(d_{n}) = \\sup_{\\delta d_{n} \\in D} \\frac{ \\Vert \\delta x_{n} \\Vert / \\Vert x_{n} \\Vert}{\\Vert \\delta d_{n} \\Vert / \\Vert d_{n} \\Vert},\n",
    "\\end{equation}\n",
    "and \n",
    "\\begin{equation}\n",
    " K_{abs,n}(d_{n}) = \\sup_{\\delta d_{n} \\in D} \\frac{\\Vert \\delta x_{n} \\Vert}{\\Vert \\delta d_{n} \\Vert}.\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Asymptotic condition numbers are defined as follows:\n",
    "\\begin{align}\n",
    "K^{num}(d_{n}) &= \\lim_{k\\rightarrow \\infty} \\sup_{n\\ge k} K_{n}(d_{n}) \\\\\n",
    "K^{num}_{\\text{abs}}(d_{n}) &= \\lim_{k\\rightarrow \\infty} \\sup_{n\\ge k} K_{\\text{abs},n}(d_{n})\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Stability of Numerical Methods\n",
    "\n",
    "The numerical method is said to be **well conditioned** if $K^{num}$ is \"small\" for any admissible datum $d_{n}$; **ill-conditioned** otherwise.\n",
    "\n",
    "A more formal way of defining the convergence of a numerical method is as follows:\n",
    "A numerical method \n",
    "\\begin{equation}\n",
    "F_{n}(x_{n}, d_{n})=0 \\quad n \\ge 1\n",
    "\\end{equation}\n",
    "is **convergent** if and only if\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "&^{\\forall}\\epsilon > 0, \\quad ^{\\exists}n_{0}(\\epsilon), \\quad ^{\\exists}\\delta(n_{0},\\epsilon) > 0 \\quad \\text{such that} \\\\\n",
    "&^{\\forall}n \\gt n_{0}(\\epsilon), \\quad ^{\\forall}\\Vert \\delta d_{n} \\Vert < \\delta(n_{0},\\epsilon), \\quad\n",
    "\\Vert x(d) - x_{n}(d+\\delta d_{n}) \\Vert \\le \\epsilon.\n",
    "\\end{split}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "What would matter more to us is how to measure the convergence, in other words, how to measure the error of an approximate solution.\n",
    "\n",
    "- **Absolute error**: $E(x_{n}) = \\Vert x - x_{n} \\Vert $\n",
    "- **Releative error**: $E_{rel}(x_{n}) = \\Vert x-x_{n} \\Vert / \\Vert x \\Vert $ if $\\Vert x \\Vert \\neq 0 $\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sources of Error\n",
    "\n",
    "- A physical problem (PP), of which solution is denoted as $x_{ph}$\n",
    "- $F(x,d)=0$: A mathematical model of (PP)\n",
    "- $F_{n}(\\hat{x}_{n},d_{n})=0$: A computational model for PP\n",
    "\n",
    "The error associated with the computational model ($e$) is\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "e &= e_{m} + e_{c} \\\\\n",
    "  &= (x-x_{ph}) + (\\hat{x}_{n}-x),\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "where \n",
    "\n",
    "- $e_{m}$: Error of mathematical model $+$ error in data\n",
    "- $e_{c}$: Discretization error ($e_{n}$) $+$ numerical algorithm $+$ roundoff error ($e_{a}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Sources of Error\n",
    "\n",
    "With the above the notations, we can summarize the sources of errors as in the next figure from Quarteroni (2000):\n",
    "\n",
    "<img src=\"./Figures/Quarteroni_Fig2.1.PNG\" width=480 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Other criteria for good numerical methods\n",
    "Of course, convergence is of the prime importance for a numerical method, the following concepts are also considered when choosing or developing a numerical method:\n",
    "\n",
    "- **Accuracy**: $e$ is small with respect to a fixed tolerance. \n",
    "    - Usually quantified as $e_{n}$ with respect to the discretization characteristic parameter\n",
    "    - e.g., the largest grid spacing between the discretization nodese.g., $e_{n} \\sim h^{2}$.\n",
    "- **Reliability**: $e$ is likely to be below a certain tolerance. Needs testing (benchmarking).\n",
    "- **Efficiency**: The computational complexity needed to control the error is as small as possible.\n",
    "    - **Complexity of an algorithm**: A measure of execution time\n",
    "    - **Complexity of a problem**: The complexity of the most efficient among the algorithms for the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Machine representation of numbers\n",
    "\n",
    "### The *positional representation* of a real number\n",
    "\n",
    "\\begin{equation}\n",
    "x_{\\beta} = (-1)^{s} \\left[ x_{n}\\, x_{n-1}\\, \\cdots\\, x_{1}\\, x_{0}.x_{-1}\\, x_{-2}\\,\\cdots x_{-m} \\right],\n",
    "\\end{equation}\n",
    "\n",
    "where $\\beta$ is *base*, $s$ determines the sign (0: $+$, 1: $-$), and the point is the decimal point if $\\beta=10$ or the binary point if $\\beta=2$.\n",
    "\n",
    "This representation can be also written as\n",
    "\\begin{equation}\n",
    "x_{\\beta} = (-1)^{s}\\left( \\sum_{k=-m}^{n} x_{k}\\beta^{k} \\right),\n",
    "\\end{equation}\n",
    "where $0\\le x_{k} \\lt \\beta$.\n",
    "\n",
    "#### Example\n",
    "\n",
    "\\begin{matrix}\n",
    "x_{10} &= 425.33 = &4\\cdot 10^{2} &+ 2\\cdot 10 &+  5 &+  3\\cdot 10^{-1} &+ 3\\cdot 10^{-2} \\\\\n",
    "x_{6}  &= 425.33 = &4\\cdot 6^{2}  &+ 2\\cdot 6  &+  5 &+  3\\cdot 6^{-1}  &+ 3\\cdot 6^{-2}\n",
    "\\end{matrix}\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "x_{10} &= 1/3 = 0.3333\\cdots = 0.\\bar{3} \\\\\n",
    "x_{3}  &= 1/3 = 0.1\n",
    "\\end{split}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The *fixed-point* number system\n",
    "\n",
    "Let's say we have $N$ memory positions in a computer. Then the **fixed-point system** goes as follows:\n",
    "\\begin{equation}\n",
    "x=\\underbrace{(-1)^{s}}_\\text{takes 1 space} [ \\underbrace{a_{n-2}\\, a_{n-3}\\, \\cdots\\, a_{k}}_\\text{N-1-k}.\\underbrace{a_{k-1}\\, \\cdots a_{0}}_\\text{k} ].\n",
    "\\end{equation}\n",
    "An equivalent summation expression is\n",
    "\\begin{equation}\n",
    "x=(-1)^{s} \\underbrace{\\beta^{-k}}_\\text{a fixed scaling factor} \\sum_{j=0}^{N-2} a_{j}\\beta^{j},\n",
    "\\end{equation}\n",
    "Since $k$ is a fixed number, minimum and maximum of the representable numbers are very limited.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The *floating-point* number system\n",
    "\n",
    "Let's consider another number representation:\n",
    "\\begin{equation}\n",
    "x = (-1)^{s} (0.a_{1}\\, a_{2}\\, \\cdots\\, a_{t})\\beta^{e} = (-1)^{s}\\, m \\, \\beta^{e-t},\n",
    "\\end{equation}\n",
    "where $t \\in \\mathbb{N}$ is the number of allowed significant digits $a_{i}$, $m=a_{1}\\, a_{2}\\, \\cdots\\, a_{t}$ an integer number called \"mantissa\" ($0 \\le m \\le \\beta^{t}-1$) and $e$ an integer called \"exponent\" ($L\\le e \\le U$ and $L\\lt 0, U\\gt 0$).\n",
    "\n",
    "- **single precision**: N = 32 bits\n",
    "<img src=\"./Figures/Quarteroni_singleprecision.PNG\" width=400 />\n",
    "\n",
    "- **double precision**: N = 64 bits\n",
    "\n",
    "<img src=\"./Figures/Quarteroni_doubleprecision.PNG\" width=600 />\n",
    "\n",
    "Let's denote the set of **floating-point numbers** with $t$ significant digits, base $\\beta \\ge 2$, $0 \\le a_{i} \\le \\beta-1$, and range $(L,U)$ with $L \\le e \\le U$ by\n",
    "\\begin{equation}\n",
    "\\mathbb{F}(\\beta, t, L, U) = \\{0\\} \\, \\cup \\, \\left\\{ x \\in \\mathbb{R}: x = (-1)^{s} \\beta^{e} \\sum_{i=1}^{t} a_{i}\\beta^{-i} \\right\\}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Example\n",
    "$\\beta=10$, $t$=4, $L=-1$, $U=4$. If $a_{1}$ can be 0, we end up with the following redundancy:\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "1 &= 0.1000 \\cdot 10^{1} \\\\\n",
    "  &= 0.0100 \\cdot 10^{2} \\\\\n",
    "  &= 0.0010 \\cdot 10^{3} \\\\\n",
    "  &= 0.0001 \\cdot 10^{4}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "Therefore $a_{1} \\neq 0$ for a unique representation and this representation is called *normalized*.\n",
    "Read pp 47-48 and Example 2.11 of Quarteroni (2000) for further discussion on normalized and *denormalized* representations.\n",
    "\n",
    "\n",
    "We can see immediately that \n",
    "\n",
    "- if $x \\in \\mathbb{F}(\\beta, t, L, U)$, then $-x \\in \\mathbb{F}$.\n",
    "- $x_{min} = \\beta^{L-1} \\le |x| \\le \\beta^{U}(1-\\beta^{-t})=x_{max}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Example\n",
    "Find all the positivie numbers in the set $\\mathbb{F}(2,3,-1,2)$.\n",
    "\n",
    "Verify that $x_{min}=\\beta^{L-1}=2^{-2}=1/4$ and $x_{max}=\\beta^{U}(1-\\beta^{-t})=2^{2}(1-s^{-3})=7/2$.\n",
    "\n",
    "\n",
    "The *standard* floating-point numbers are \n",
    "\n",
    "- $\\mathbb{F}(2,24,-125,128)$ for the single precision\n",
    "- $\\mathbb{F}(2,53,-1021,1024)$ for the double precision"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
